{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suryanshagarwal30/Deeplearning_lab/blob/main/Overview_of_Colaboratory_Features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from PIL import Image\n"
      ],
      "metadata": {
        "id": "bB6tl97RdY6P"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create directories for images and annotations\n",
        "os.makedirs('/content/coco/images', exist_ok=True)\n",
        "os.makedirs('/content/coco/annotations', exist_ok=True)\n"
      ],
      "metadata": {
        "id": "gKsQt93qdvfP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download training images\n",
        "!wget -c http://images.cocodataset.org/zips/train2017.zip -P /content/coco/images\n",
        "\n",
        "# Download validation images\n",
        "!wget -c http://images.cocodataset.org/zips/val2017.zip -P /content/coco/images\n",
        "\n",
        "# Download annotations\n",
        "!wget -c http://images.cocodataset.org/annotations/annotations_trainval2017.zip -P /content/coco/annotations\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKgVNFfGdx1E",
        "outputId": "155b53f6-0799-4c53-80b1-47c5c8977cf1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-06 07:19:53--  http://images.cocodataset.org/zips/train2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 3.5.12.95, 54.231.203.49, 3.5.29.47, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|3.5.12.95|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19336861798 (18G) [application/zip]\n",
            "Saving to: ‘/content/coco/images/train2017.zip’\n",
            "\n",
            "train2017.zip       100%[===================>]  18.01G  93.0MB/s    in 3m 38s  \n",
            "\n",
            "2025-05-06 07:23:32 (84.6 MB/s) - ‘/content/coco/images/train2017.zip’ saved [19336861798/19336861798]\n",
            "\n",
            "--2025-05-06 07:23:32--  http://images.cocodataset.org/zips/val2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 3.5.25.126, 16.182.108.137, 3.5.12.237, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|3.5.25.126|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 815585330 (778M) [application/zip]\n",
            "Saving to: ‘/content/coco/images/val2017.zip’\n",
            "\n",
            "val2017.zip         100%[===================>] 777.80M  95.0MB/s    in 8.1s    \n",
            "\n",
            "2025-05-06 07:23:40 (95.5 MB/s) - ‘/content/coco/images/val2017.zip’ saved [815585330/815585330]\n",
            "\n",
            "--2025-05-06 07:23:40--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.123.121, 52.216.51.129, 16.182.103.185, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.123.121|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252907541 (241M) [application/zip]\n",
            "Saving to: ‘/content/coco/annotations/annotations_trainval2017.zip’\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.19M  96.3MB/s    in 2.5s    \n",
            "\n",
            "2025-05-06 07:23:42 (96.3 MB/s) - ‘/content/coco/annotations/annotations_trainval2017.zip’ saved [252907541/252907541]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip training images\n",
        "!unzip -q /content/coco/images/train2017.zip -d /content/coco/images\n",
        "\n",
        "# Unzip validation images\n",
        "!unzip -q /content/coco/images/val2017.zip -d /content/coco/images\n",
        "\n",
        "# Unzip annotations\n",
        "!unzip -q /content/coco/annotations/annotations_trainval2017.zip -d /content/coco/annotations\n"
      ],
      "metadata": {
        "id": "TJQz7FBZeu1N"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List training images\n",
        "!ls /content/coco/images/train2017 | head\n",
        "\n",
        "# List validation images\n",
        "!ls /content/coco/images/val2017 | head\n",
        "\n",
        "# List annotation files\n",
        "!ls /content/coco/annotations\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd2et4Fxfn40",
        "outputId": "0dc1d440-ff5f-44e8-8cbb-c123315f0210"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "000000000009.jpg\n",
            "000000000025.jpg\n",
            "000000000030.jpg\n",
            "000000000034.jpg\n",
            "000000000036.jpg\n",
            "000000000042.jpg\n",
            "000000000049.jpg\n",
            "000000000061.jpg\n",
            "000000000064.jpg\n",
            "000000000071.jpg\n",
            "000000000139.jpg\n",
            "000000000285.jpg\n",
            "000000000632.jpg\n",
            "000000000724.jpg\n",
            "000000000776.jpg\n",
            "000000000785.jpg\n",
            "000000000802.jpg\n",
            "000000000872.jpg\n",
            "000000000885.jpg\n",
            "000000001000.jpg\n",
            "annotations  annotations_trainval2017.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download test images\n",
        "!wget -c http://images.cocodataset.org/zips/test2017.zip -P /content/coco/images\n",
        "\n",
        "# Unzip test images\n",
        "!unzip -q /content/coco/images/test2017.zip -d /content/coco/images\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFOSHAlsfr86",
        "outputId": "237eb26b-be78-4f57-a176-e328471c2594"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-06 07:28:13--  http://images.cocodataset.org/zips/test2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.108.83, 54.231.235.113, 52.217.13.212, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.108.83|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6646970404 (6.2G) [application/zip]\n",
            "Saving to: ‘/content/coco/images/test2017.zip’\n",
            "\n",
            "test2017.zip        100%[===================>]   6.19G  61.5MB/s    in 1m 40s  \n",
            "\n",
            "2025-05-06 07:29:54 (63.2 MB/s) - ‘/content/coco/images/test2017.zip’ saved [6646970404/6646970404]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    return img, image_path\n"
      ],
      "metadata": {
        "id": "KikR-9PqdfZm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip -P /content/coco/annotations\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uadQCDZhWDW",
        "outputId": "419c2cad-7f22-4621-bd1b-6cdbfcb30b75"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-06 07:35:39--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.160.105, 52.217.108.212, 52.217.172.169, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.160.105|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252907541 (241M) [application/zip]\n",
            "Saving to: ‘/content/coco/annotations/annotations_trainval2017.zip’\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.19M  96.2MB/s    in 2.5s    \n",
            "\n",
            "2025-05-06 07:35:42 (96.2 MB/s) - ‘/content/coco/annotations/annotations_trainval2017.zip’ saved [252907541/252907541]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o /content/coco/annotations/annotations_trainval2017.zip -d /content/coco/annotations/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sH2gBTo_hbNN",
        "outputId": "4d02a9b4-12c8-44e6-81e2-6b0db31c74ab"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/coco/annotations/annotations_trainval2017.zip\n",
            "  inflating: /content/coco/annotations/annotations/instances_train2017.json  \n",
            "  inflating: /content/coco/annotations/annotations/instances_val2017.json  \n",
            "  inflating: /content/coco/annotations/annotations/captions_train2017.json  \n",
            "  inflating: /content/coco/annotations/annotations/captions_val2017.json  \n",
            "  inflating: /content/coco/annotations/annotations/person_keypoints_train2017.json  \n",
            "  inflating: /content/coco/annotations/annotations/person_keypoints_val2017.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Move the file to the desired directory\n",
        "shutil.move('/content/coco/annotations/annotations/captions_train2017.json',\n",
        "            '/content/coco/annotations/captions_train2017.json')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Zq_40roLizYv",
        "outputId": "d700cf87-7445-4223-acf3-8bcb89261d7d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/coco/annotations/captions_train2017.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# Paths\n",
        "annotations_path = '/content/coco/annotations/captions_train2017.json'\n",
        "images_path = '/content/coco/images/train2017/'\n",
        "subset_images_path = '/content/coco/images/train2017_subset/'\n",
        "subset_annotations_path = '/content/coco/annotations/captions_train2017_subset.json'\n",
        "\n",
        "# Create subset directory\n",
        "os.makedirs(subset_images_path, exist_ok=True)\n",
        "\n",
        "# Load annotations\n",
        "with open(annotations_path, 'r') as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "# Get unique image IDs\n",
        "image_ids = list(set([ann['image_id'] for ann in annotations['annotations']]))\n",
        "\n",
        "# Select 1000 random image IDs\n",
        "subset_image_ids = random.sample(image_ids, 1000)\n",
        "\n",
        "# Copy selected images to subset directory\n",
        "for img in annotations['images']:\n",
        "    if img['id'] in subset_image_ids:\n",
        "        src = os.path.join(images_path, img['file_name'])\n",
        "        dst = os.path.join(subset_images_path, img['file_name'])\n",
        "        shutil.copyfile(src, dst)\n",
        "\n",
        "# Filter annotations for selected images\n",
        "subset_annotations = {\n",
        "    'info': annotations['info'],\n",
        "    'licenses': annotations['licenses'],\n",
        "    'images': [img for img in annotations['images'] if img['id'] in subset_image_ids],\n",
        "    'annotations': [ann for ann in annotations['annotations'] if ann['image_id'] in subset_image_ids]\n",
        "}\n",
        "\n",
        "# Save subset annotations\n",
        "with open(subset_annotations_path, 'w') as f:\n",
        "    json.dump(subset_annotations, f)\n"
      ],
      "metadata": {
        "id": "T_ZaN6RBdjkr"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the InceptionV3 model\n",
        "image_model = InceptionV3(include_top=False, weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "cnn_model = tf.keras.Model(new_input, hidden_layer)\n",
        "\n",
        "# Function to load and preprocess image\n",
        "def load_image(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    img = preprocess_input(img)\n",
        "    return img, img_path\n",
        "\n",
        "# Extract features and save them\n",
        "image_folder = '/content/coco/images/train2017_subset'\n",
        "encode_train = sorted(os.listdir(image_folder))\n",
        "image_dataset = tf.data.Dataset.from_tensor_slices([image_folder + img for img in encode_train])\n",
        "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n",
        "\n",
        "# Directory to save the extracted features\n",
        "features_path = '/content/coco/features/'\n",
        "os.makedirs(features_path, exist_ok=True)\n",
        "\n",
        "for img, path in tqdm(image_dataset):\n",
        "    batch_features = cnn_model(img)\n",
        "    batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "\n",
        "    for bf, p in zip(batch_features, path):\n",
        "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "        np.save(features_path + os.path.basename(path_of_feature).split('.')[0], bf.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4IUZ0X6jfq0",
        "outputId": "c7907141-f61c-4f3d-b084-760e50eb5393"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:48<00:00,  2.56it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the subset annotations\n",
        "with open('/content/coco/annotations/captions_train2017_subset.json', 'r') as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "# Create a list of all captions\n",
        "all_captions = []\n",
        "image_paths = []\n",
        "\n",
        "for annot in annotations['annotations']:\n",
        "    caption = '<start> ' + annot['caption'] + ' <end>'\n",
        "    image_id = annot['image_id']\n",
        "    image_file = [img['file_name'] for img in annotations['images'] if img['id'] == image_id][0]\n",
        "    full_image_path = os.path.join(image_folder, image_file)\n",
        "    all_captions.append(caption)\n",
        "    image_paths.append(full_image_path)\n",
        "\n",
        "# Tokenize the captions\n",
        "top_k = 5000\n",
        "tokenizer = Tokenizer(num_words=top_k, oov_token='<unk>', filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "train_seqs = tokenizer.texts_to_sequences(all_captions)\n",
        "\n",
        "# Pad the sequences\n",
        "cap_vector = pad_sequences(train_seqs, padding='post')\n",
        "\n",
        "# Save the tokenizer for later use\n",
        "import pickle\n",
        "with open('tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n"
      ],
      "metadata": {
        "id": "ZKeltRuvjvMg"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the encoder\n",
        "class CNN_Encoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x\n",
        "\n",
        "# Define the attention mechanism\n",
        "class BahdanauAttention(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, features, hidden):\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
        "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "        context_vector = attention_weights * features\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "# Define the decoder\n",
        "class RNN_Decoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim, units, vocab_size):\n",
        "        super(RNN_Decoder, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "    def call(self, x, features, hidden):\n",
        "        # Attention mechanism\n",
        "        context_vector, attention_weights = self.attention(features, hidden)\n",
        "\n",
        "        # Embedding\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Concatenate context vector and embedding\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        # GRU with initial hidden state\n",
        "        output, state = self.gru(x, initial_state=hidden)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = self.fc1(output)\n",
        "        x = tf.reshape(x, (-1, x.shape[2]))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x, state, attention_weights\n",
        "\n",
        "    def reset_state(self, batch_size):\n",
        "        return tf.zeros((batch_size, self.units))\n"
      ],
      "metadata": {
        "id": "XVXTxrhcjyYE"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the encoder\n",
        "class CNN_Encoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x\n",
        "\n",
        "# Define the attention mechanism\n",
        "class BahdanauAttention(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, features, hidden):\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
        "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "        context_vector = attention_weights * features\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "# Define the decoder\n",
        "class RNN_Decoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim, units, vocab_size):\n",
        "        super(RNN_Decoder, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "    def call(self, x, features, hidden):\n",
        "        # Attention mechanism\n",
        "        context_vector, attention_weights = self.attention(features, hidden)\n",
        "\n",
        "        # Embedding\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Concatenate context vector and embedding\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        # GRU with initial hidden state\n",
        "        outputs = self.gru(x, initial_state=hidden)\n",
        "\n",
        "        # Ensure only two outputs are unpacked\n",
        "        if isinstance(outputs, (tuple, list)) and len(outputs) > 2:\n",
        "            output, state = outputs[:2]  # Take only the first two values\n",
        "        else:\n",
        "            output, state = outputs\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = self.fc1(output)\n",
        "        x = tf.reshape(x, (-1, x.shape[2]))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x, state, attention_weights\n",
        "\n",
        "    def reset_state(self, batch_size):\n",
        "        return tf.zeros((batch_size, self.units))\n",
        "\n",
        "# Corrected training loop\n",
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "\n",
        "        # Initialize the hidden state\n",
        "        hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "\n",
        "        # Pass the image features through the encoder\n",
        "        features = encoder(img_tensor)\n",
        "\n",
        "        # Set the decoder input to the <start> token\n",
        "        dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            for i in range(1, target.shape[1]):\n",
        "                # Pass the features through the decoder\n",
        "                predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
        "\n",
        "                loss += loss_function(target[:, i], predictions)\n",
        "\n",
        "                # Use teacher forcing\n",
        "                dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "        total_loss += (loss / int(target.shape[1]))\n",
        "\n",
        "        # Apply gradients\n",
        "        trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "    print(f'Epoch {epoch+1} Loss {total_loss/len(dataset):.6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "sIR-Bks4pOZH",
        "outputId": "b6ba4fc9-0b04-4c8f-80be-8d4c58aab105"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling RNN_Decoder.call().\n\n\u001b[1mtoo many values to unpack (expected 2)\u001b[0m\n\nArguments received by RNN_Decoder.call():\n  • x=tf.Tensor(shape=(64, 1), dtype=int32)\n  • features=tf.Tensor(shape=(64, 64, 256), dtype=float32)\n  • hidden=tf.Tensor(shape=(64, 512), dtype=float32)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-dd00926a9158>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;31m# Pass the features through the decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-109318fbc484>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, features, hidden)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# GRU with initial hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# Fully connected layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling RNN_Decoder.call().\n\n\u001b[1mtoo many values to unpack (expected 2)\u001b[0m\n\nArguments received by RNN_Decoder.call():\n  • x=tf.Tensor(shape=(64, 1), dtype=int32)\n  • features=tf.Tensor(shape=(64, 64, 256), dtype=float32)\n  • hidden=tf.Tensor(shape=(64, 512), dtype=float32)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set parameters\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "vocab_size = top_k + 1\n",
        "features_shape = 2048\n",
        "attention_features_shape = 64\n",
        "\n",
        "# Initialize the encoder and decoder\n",
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "# Define the loss function\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "# Load the numpy files\n",
        "def map_func(img_name, cap):\n",
        "    img_tensor = np.load(features_path + os.path.basename(img_name.decode('utf-8')).split('.')[0] + '.npy')\n",
        "    return img_tensor, cap\n",
        "\n",
        "# Create the dataset\n",
        "import tensorflow as tf\n",
        "\n",
        "# Convert the lists to tensors\n",
        "img_name_vector = image_paths\n",
        "cap_vector = cap_vector\n",
        "\n",
        "# Create a dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((img_name_vector, cap_vector))\n",
        "\n",
        "# Use map to load the numpy files\n",
        "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
        "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
        "          num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Shuffle and batch\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Training loop\n",
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "\n",
        "        # Initialize the hidden state\n",
        "        hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "\n",
        "        # Pass the image features through the encoder\n",
        "        features = encoder(img_tensor)\n",
        "\n",
        "        # Set the decoder input to the <start> token\n",
        "        dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            for i in range(1, target.shape[1]):\n",
        "                # Pass the features through the decoder\n",
        "                predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "\n",
        "                loss += loss_function(target[:, i], predictions)\n",
        "\n",
        "                # Use teacher forcing\n",
        "                dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "        total_loss += (loss / int(target.shape[1]))\n",
        "\n",
        "        # Apply gradients\n",
        "        trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "    print(f'Epoch {epoch+1} Loss {total_loss/len(dataset):.6f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "OdlW1a7Ij59G",
        "outputId": "c2a6c0cb-f882-4027-9171-59324d185d48"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling RNN_Decoder.call().\n\n\u001b[1mtoo many values to unpack (expected 2)\u001b[0m\n\nArguments received by RNN_Decoder.call():\n  • x=tf.Tensor(shape=(64, 1), dtype=int32)\n  • features=tf.Tensor(shape=(64, 64, 256), dtype=float32)\n  • hidden=tf.Tensor(shape=(64, 512), dtype=float32)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-2e8c6468fe27>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;31m# Pass the features through the decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-109318fbc484>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, features, hidden)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# GRU with initial hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# Fully connected layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling RNN_Decoder.call().\n\n\u001b[1mtoo many values to unpack (expected 2)\u001b[0m\n\nArguments received by RNN_Decoder.call():\n  • x=tf.Tensor(shape=(64, 1), dtype=int32)\n  • features=tf.Tensor(shape=(64, 64, 256), dtype=float32)\n  • hidden=tf.Tensor(shape=(64, 512), dtype=float32)"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Overview of Colaboratory Features",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}